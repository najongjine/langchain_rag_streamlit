# https://najongjine-langchain-rag-streaml-langchain-rag-streamlit-jetrtl.streamlit.app/

import os
import requests
import streamlit as st
#from langchain.embeddings import HuggingFaceEmbeddings
#from langchain.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
import google.generativeai as genai
from pathlib import Path

# â–¶ ìŠ¤í¬ë¦½íŠ¸ì™€ ë™ì¼í•œ ê²½ë¡œ ê¸°ì¤€ ì„¤ì •
BASE_DIR = Path(__file__).resolve().parent
FAISS_DIR = BASE_DIR / "faiss_data"   # ê°™ì€ í´ë”ì— 'faiss_data' ë””ë ‰í† ë¦¬ë¡œ ê°€ì •
FAISS_INDEX = FAISS_DIR / "index.faiss"
FAISS_STORE = FAISS_DIR / "index.pkl"  # LangChain FAISSëŠ” ë³´í†µ .faiss + .pkl ì„¸íŠ¸

"""
# Hugging Face ì €ì¥ì†Œì—ì„œ FAISS íŒŒì¼ ë‹¤ìš´ë¡œë“œ
@st.cache_resource
def download_faiss_from_hf():
    hf_url = "https://huggingface.co/datasets/WildOjisan/test_embedding/resolve/main"
    os.makedirs("faiss_data", exist_ok=True)

    for fname in ["index.faiss", "index.pkl"]:
        file_path = f"faiss_data/{fname}"
        if not os.path.exists(file_path):
            with open(file_path, "wb") as f:
                f.write(requests.get(f"{hf_url}/{fname}").content)
"""
@st.cache_resource
def ensure_local_faiss():
    """ë¡œì»¬ faiss íŒŒì¼ ì¡´ì¬ í™•ì¸. ì—†ìœ¼ë©´ ì—ëŸ¬ í›„ ì¤‘ë‹¨."""
    missing = []
    if not FAISS_INDEX.exists():
        missing.append(str(FAISS_INDEX))
    if not FAISS_STORE.exists():
        missing.append(str(FAISS_STORE))

    if missing:
        st.error(
            "FAISS íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"
            f"ë‹¤ìŒ ê²½ë¡œì— íŒŒì¼ì„ ë‘ì„¸ìš”:\n- {FAISS_INDEX}\n- {FAISS_STORE}"
        )
        st.stop()

# FAISS + ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
@st.cache_resource
def load_vector_db():
    ensure_local_faiss()
    embedding_model = HuggingFaceEmbeddings(
        model_name="intfloat/multilingual-e5-base",
        encode_kwargs={"normalize_embeddings": True}
    )
    vector_db = FAISS.load_local(
        folder_path=str(FAISS_DIR),
        embeddings=embedding_model,
        allow_dangerous_deserialization=True,  # ì‹ ë¢° ê°€ëŠ¥í•œ í™˜ê²½ì—ì„œë§Œ ì‚¬ìš©
    )
    return vector_db.as_retriever()

# Gemini API ì„¤ì •
@st.cache_resource
def load_gemini_model():
    genai.configure(api_key="AIzaSyDy8om1vG9J7kEECBSvLKzvXC1FuF-0aHE")  # ğŸ”‘ ë³¸ì¸ì˜ Gemini API í‚¤ë¡œ êµì²´!
    return genai.GenerativeModel("gemini-2.5-flash")

# RAG ì‘ë‹µ í•¨ìˆ˜
def gemini_rag_answer(query, retriever, model):
    docs = retriever.get_relevant_documents(query)
    context = "\n".join([doc.page_content for doc in docs])

    prompt = f"""
ë‹¹ì‹ ì€ ë¬¸ì„œ ê¸°ë°˜ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” AIì…ë‹ˆë‹¤.
ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•´ì„œ ì§ˆë¬¸ì— ë‹µí•˜ì‹­ì‹œì˜¤:

ë¬¸ì„œ:
{context}

ì§ˆë¬¸:
{query}

ë‹µë³€:
"""
    response = model.generate_content(prompt)
    return response.text

# Streamlit ì¸í„°í˜ì´ìŠ¤
st.title("ğŸ“š ë¬¸ì„œ ê²€ìƒ‰ ê¸°ë°˜ Gemini ì±—ë´‡")

#download_faiss_from_hf()
retriever = load_vector_db()
model = load_gemini_model()

query = st.text_input("â“ ê¶ê¸ˆí•œ ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”:")

if query:
    with st.spinner("ğŸ” ê²€ìƒ‰ ì¤‘..."):
        answer = gemini_rag_answer(query, retriever, model)
    st.markdown("### ğŸ¤– Geminiì˜ ë‹µë³€:")
    st.write(answer)
