import os
import requests
import streamlit as st
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
import google.generativeai as genai

# Hugging Face ì €ì¥ì†Œì—ì„œ FAISS íŒŒì¼ ë‹¤ìš´ë¡œë“œ
@st.cache_resource
def download_faiss_from_hf():
    hf_url = "https://huggingface.co/datasets/WildOjisan/test_embedding/resolve/main"
    os.makedirs("faiss_data", exist_ok=True)

    for fname in ["index.faiss", "index.pkl"]:
        file_path = f"faiss_data/{fname}"
        if not os.path.exists(file_path):
            with open(file_path, "wb") as f:
                f.write(requests.get(f"{hf_url}/{fname}").content)

# FAISS + ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
@st.cache_resource
def load_vector_db():
    embedding_model = HuggingFaceEmbeddings(
        model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    )
    vector_db = FAISS.load_local("faiss_data", embedding_model, allow_dangerous_deserialization=True)
    return vector_db.as_retriever()

# Gemini API ì„¤ì •
@st.cache_resource
def load_gemini_model():
    genai.configure(api_key="YOUR_GEMINI_API_KEY")  # ğŸ”‘ ë³¸ì¸ì˜ Gemini API í‚¤ë¡œ êµì²´!
    return genai.GenerativeModel("gemini-2.5-flash")

# RAG ì‘ë‹µ í•¨ìˆ˜
def gemini_rag_answer(query, retriever, model):
    docs = retriever.get_relevant_documents(query)
    context = "\n".join([doc.page_content for doc in docs])

    prompt = f"""
ë‹¹ì‹ ì€ ë¬¸ì„œ ê¸°ë°˜ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” AIì…ë‹ˆë‹¤.
ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•´ì„œ ì§ˆë¬¸ì— ë‹µí•˜ì‹­ì‹œì˜¤:

ë¬¸ì„œ:
{context}

ì§ˆë¬¸:
{query}

ë‹µë³€:
"""
    response = model.generate_content(prompt)
    return response.text

# Streamlit ì¸í„°í˜ì´ìŠ¤
st.title("ğŸ“š ë¬¸ì„œ ê²€ìƒ‰ ê¸°ë°˜ Gemini ì±—ë´‡")

download_faiss_from_hf()
retriever = load_vector_db()
model = load_gemini_model()

query = st.text_input("â“ ê¶ê¸ˆí•œ ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”:")

if query:
    with st.spinner("ğŸ” ê²€ìƒ‰ ì¤‘..."):
        answer = gemini_rag_answer(query, retriever, model)
    st.markdown("### ğŸ¤– Geminiì˜ ë‹µë³€:")
    st.write(answer)
